Part 2- What are some ways websites can prevent scraping data?
The first thing that comes to mind regarding this are bots. As scraping directly targets the src, a bot could be set in place that prevents third party programs to access src material(HTML). Another way is to simply prevent the rate at which a particular IP address can make requests. Captchas are also a common measure to prevent scraping.


Part 3- What are some ways to overcome the above mentioned?
The first thing that comes to mind here is to constantly change IP via Proxy to tackle the latter defensive measure. Some of the other defensive measures are difficult to overcome. I believe a bot can be overcome by changing things such as monitor resolution and click rate for instance. Sometimes bots are able to keep track of someone's browser fingerprint by keeping track of such details.
